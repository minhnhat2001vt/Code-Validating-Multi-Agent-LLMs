{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 2 agents\n",
    "import getpass\n",
    "\n",
    "API_key = getpass.getpass(\"OpenAI API Key:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define retrieval tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting protobuf==3.20.3\n",
      "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
      "Downloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
      "Installing collected packages: protobuf\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 5.29.1\n",
      "    Uninstalling protobuf-5.29.1:\n",
      "      Successfully uninstalled protobuf-5.29.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.10.0 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\n",
      "opentelemetry-proto 1.29.0 requires protobuf<6.0,>=5.0, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed protobuf-3.20.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install protobuf==3.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from pypdf) (4.12.2)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-5.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rapidocr-onnxruntime\n",
      "  Downloading rapidocr_onnxruntime-1.4.3-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting pyclipper>=1.2.0 (from rapidocr-onnxruntime)\n",
      "  Downloading pyclipper-1.3.0.post6-cp310-cp310-macosx_10_9_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: opencv-python>=4.5.1.48 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from rapidocr-onnxruntime) (4.5.5)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.19.5 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from rapidocr-onnxruntime) (1.26.4)\n",
      "Requirement already satisfied: six>=1.15.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from rapidocr-onnxruntime) (1.16.0)\n",
      "Collecting Shapely!=2.0.4,>=1.7.1 (from rapidocr-onnxruntime)\n",
      "  Downloading shapely-2.0.6-cp310-cp310-macosx_10_9_x86_64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: PyYAML in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from rapidocr-onnxruntime) (6.0.1)\n",
      "Requirement already satisfied: Pillow in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from rapidocr-onnxruntime) (9.4.0)\n",
      "Requirement already satisfied: onnxruntime>=1.7.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from rapidocr-onnxruntime) (1.19.2)\n",
      "Requirement already satisfied: tqdm in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from rapidocr-onnxruntime) (4.66.2)\n",
      "Requirement already satisfied: coloredlogs in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.3.7)\n",
      "Requirement already satisfied: packaging in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (24.0)\n",
      "Requirement already satisfied: protobuf in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (5.29.1)\n",
      "Requirement already satisfied: sympy in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.12)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.7.0->rapidocr-onnxruntime) (10.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sympy->onnxruntime>=1.7.0->rapidocr-onnxruntime) (1.3.0)\n",
      "Downloading rapidocr_onnxruntime-1.4.3-py3-none-any.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading pyclipper-1.3.0.post6-cp310-cp310-macosx_10_9_x86_64.whl (142 kB)\n",
      "Downloading shapely-2.0.6-cp310-cp310-macosx_10_9_x86_64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyclipper, Shapely, rapidocr-onnxruntime\n",
      "Successfully installed Shapely-2.0.6 pyclipper-1.3.0.post6 rapidocr-onnxruntime-1.4.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence_transformers\n",
      "  Using cached sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n",
      "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sentence_transformers) (4.66.2)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: scikit-learn in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sentence_transformers) (1.0.2)\n",
      "Requirement already satisfied: scipy in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sentence_transformers) (1.12.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sentence_transformers) (0.21.4)\n",
      "Requirement already satisfied: Pillow in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sentence_transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2024.3.1)\n",
      "Requirement already satisfied: requests in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.31.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.12.2)\n",
      "Requirement already satisfied: packaging>=20.9 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (24.0)\n",
      "Requirement already satisfied: sympy in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\n",
      "Requirement already satisfied: networkx in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.3)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence_transformers)\n",
      "  Using cached huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2023.12.25)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n",
      "  Using cached tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.4.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from scikit-learn->sentence_transformers) (3.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
      "Using cached sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n",
      "Using cached transformers-4.47.0-py3-none-any.whl (10.1 MB)\n",
      "Using cached huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_10_12_x86_64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers, sentence_transformers\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface_hub 0.21.4\n",
      "    Uninstalling huggingface_hub-0.21.4:\n",
      "      Successfully uninstalled huggingface_hub-0.21.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.39.1\n",
      "    Uninstalling transformers-4.39.1:\n",
      "      Successfully uninstalled transformers-4.39.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "chromadb 0.5.23 requires tokenizers<=0.20.3,>=0.13.2, but you have tokenizers 0.21.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed huggingface-hub-0.26.5 sentence_transformers-3.3.1 tokenizers-0.21.0 transformers-4.47.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence_transformers --default-timeout=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (24.0)\n",
      "Collecting pip\n",
      "  Downloading pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (69.2.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: wheel in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (0.35.1)\n",
      "Collecting wheel\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Downloading pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.5/72.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: wheel, setuptools, pip\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.35.1\n",
      "    Uninstalling wheel-0.35.1:\n",
      "      Successfully uninstalled wheel-0.35.1\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 69.2.0\n",
      "    Uninstalling setuptools-69.2.0:\n",
      "      Successfully uninstalled setuptools-69.2.0\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.0\n",
      "    Uninstalling pip-24.0:\n",
      "      Successfully uninstalled pip-24.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.10.0 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pip-24.3.1 setuptools-75.6.0 wheel-0.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip setuptools wheel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting grpcio==1.43.0\n",
      "  Downloading grpcio-1.43.0-cp310-cp310-macosx_10_10_universal2.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: six>=1.5.2 in /Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages (from grpcio==1.43.0) (1.16.0)\n",
      "Downloading grpcio-1.43.0-cp310-cp310-macosx_10_10_universal2.whl (4.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: grpcio\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.42.0\n",
      "    Uninstalling grpcio-1.42.0:\n",
      "      Successfully uninstalled grpcio-1.42.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.10.0 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed grpcio-1.43.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install grpcio==1.43.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.10.0 requires libclang>=13.0.0, which is not installed.\n",
      "tensorflow 2.10.0 requires tensorflow-io-gcs-filesystem>=0.23.1, which is not installed.\n",
      "tensorboard 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.29.1 which is incompatible.\n",
      "tensorflow 2.10.0 requires protobuf<3.20,>=3.9.2, but you have protobuf 5.29.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -qU \"langchain-chroma>=0.1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "def retriever(pdf_url: str, query: str) -> str:\n",
    "    docs = PyPDFLoader(pdf_url, extract_images=True).load()\n",
    "    chroma_db = Chroma.from_documents(docs, embedding=HuggingFaceBgeEmbeddings())\n",
    "\n",
    "    chroma_retriever = chroma_db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\":2}\n",
    "    )\n",
    "    retrieved_docs = chroma_retriever.invoke(query)\n",
    "\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent\n",
    "\n",
    "assistant = ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant. \"\n",
    "    \"You can help answering questions based on input documents. \"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.9, \"api_key\": API_key}]},\n",
    ")\n",
    "\n",
    "user_proxy = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Register tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import register_function\n",
    "\n",
    "register_function(\n",
    "    retriever,\n",
    "    caller=assistant,\n",
    "    executor=user_proxy,\n",
    "    name=\"retriever\",\n",
    "    description=\"A retriever who can return the relevant documents for a query based on a pdf document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "Based on https://arxiv.org/pdf/2308.10792 paper, could you tell me: What is Instruction Tuning?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_fw1O2f2ldO14D3Ew36LaKRVQ): retriever *****\u001b[0m\n",
      "Arguments: \n",
      "{\"pdf_url\":\"https://arxiv.org/pdf/2308.10792\",\"query\":\"Instruction Tuning\"}\n",
      "\u001b[32m**************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION retriever...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/macos/anaconda3/envs/DeepLearning/lib/python3.10/site-packages/autogen/agentchat/conversable_agent.py:2665: LangChainDeprecationWarning: Default values for HuggingFaceBgeEmbeddings.model_name were deprecated in LangChain 0.2.5 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceBgeEmbeddings constructor instead.\n",
      "  retval = func(*args, **kwargs)\n",
      "2024-12-13 00:03:10.729823: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_fw1O2f2ldO14D3Ew36LaKRVQ) *****\u001b[0m\n",
      "that properly cover the desired target behaviors\n",
      "is non-trivial: existing instruction datasets are\n",
      "usually limited in quantity, diversity, and creativity;\n",
      "(2) there has been an increasing concern that\n",
      "SFT only improves on tasks that are heavily\n",
      "supported in the SFT training dataset (Gudibande\n",
      "et al., 2023); and (3) there has been an intense\n",
      "criticism that SFT only captures surface-level\n",
      "patterns and styles (e.g., the output format)\n",
      "rather than comprehending and learning the task\n",
      "(Kung and Peng, 2023). Improving instruction\n",
      "adherence and handling unanticipated model\n",
      "responses remain open research problems. These\n",
      "challenges highlight the importance of further\n",
      "investigations, analysis, and summarization in this\n",
      "field, to optimize the fine-tuning process and better\n",
      "understand the behavior of instruction tuned LLMs.\n",
      "In the literature, there has been an increasing\n",
      "research interest in analysis and discussions on\n",
      "LLMs, including pre-training methods (Zhao et al.,\n",
      "2023), reasoning abilities (Huang and Chang,\n",
      "2022), downstream applications (Yang et al.,\n",
      "2023a; Sun et al., 2023b), but rarely on the topic\n",
      "of LLM instruction tuning. This survey attempts\n",
      "to fill this blank, organizing the most up-to-date\n",
      "state of knowledge on this quickly advancing field.\n",
      "Specifically,\n",
      "• Section 2 presents the general methodology\n",
      "employed in instruction tuning.\n",
      "• Section 3 outlines the construction process of\n",
      "commonly-used SFT representative datasets.\n",
      "• Section 4 presents representative instruction\n",
      "tuned models.\n",
      "• Section 5 reviews multi-modality techniques\n",
      "and datasets for instruction tuning, including\n",
      "images, speech, and video.\n",
      "• Section 6 reviews efforts to adapt LLMs to\n",
      "different domains and applications using the\n",
      "SFT strategy.\n",
      "• Section 7 reviews explorations to make\n",
      "instruction tuning more efficient, reducing the\n",
      "computational and time costs associated with\n",
      "adapting large models.\n",
      "• Section 8 presents the evaluation of SFT\n",
      "models, analysis on them, along with criticism\n",
      "against them.\n",
      "2 Methodology\n",
      "In this section, we describe the general pipeline\n",
      "employed in instruction tuning.\n",
      "2.1 Instruction Dataset Construction\n",
      "Each instance in an instruction dataset consists of\n",
      "three elements: an instruction, which is a natural\n",
      "language text sequence to specify the task (e.g.,\n",
      "write a thank-you letter to XX for XX, write a blog\n",
      "on the topic of XX , etc); an optional input which\n",
      "provides supplementary information for context;\n",
      "and an anticipated output based on the instruction\n",
      "and the input.\n",
      "There are generally two methods for\n",
      "constructing instruction datasets:\n",
      "• Data integration from annotated natural\n",
      "language datasets. In this approach,\n",
      "(instruction, output) pairs are collected from\n",
      "existing annotated natural language datasets\n",
      "by using templates to transform text-label\n",
      "pairs to (instruction, output) pairs. Datasets\n",
      "such as Flan (Longpre et al., 2023) and\n",
      "P3 (Sanh et al., 2021) are constructed based\n",
      "on the data integration strategy.\n",
      "• Generating outputs using LLMs: An alternate\n",
      "way to quickly gather the desired outputs to\n",
      "given instructions is to employ LLMs such as\n",
      "GPT-3.5-Turbo or GPT4 instead of manually\n",
      "collecting the outputs. Instructions can come\n",
      "from two sources: (1) manually collected; or\n",
      "(2) expanded based a small handwritten seed\n",
      "instructions using LLMs. Next, the collected\n",
      "instructions are fed to LLMs to obtain outputs.\n",
      "Datasets such as InstructWild (Xue et al.,\n",
      "2023) and Self-Instruct (Wang et al., 2022c)\n",
      "are geneated following this approach.\n",
      "For multi-turn conversational SFT datasets, we\n",
      "can have large language models self-play different\n",
      "roles (user and AI assistant) to generate messages\n",
      "in a conversational format (Xu et al., 2023b).\n",
      "2.2 Instruction Tuning / Supervised\n",
      "Fine-tuning\n",
      "Based on the collected SFT dataset, a pretrained\n",
      "model can be directly fune-tuned in a fully-\n",
      "supervised manner, where given the instruction and\n",
      "the input, the model is trained by predicting each\n",
      "token in the output sequentially.\n",
      "3 Datasets\n",
      "In this section, we detail instruction tuning datasets\n",
      "in the community, categorizing them into three\n",
      "classes: (1) Human-crafted Data, (2) Synthetic\n",
      "Data via Distillation, and (3) Synthetic Data via\n",
      "\n",
      "text-label instruction-\n",
      "output\n",
      "seed \n",
      "instructions\n",
      "more \n",
      "instructions output\n",
      "LLM\n",
      "Supervised \n",
      "Finetuning\n",
      "templates\n",
      "ChatGPT \n",
      "& GPT4\n",
      "Step1: Instruction Dataset Construction Step2: Instruction Tuning\n",
      "LLM\n",
      "ChatGPT \n",
      "& GPT4\n",
      "Figure 1: General pipeline of instruction tuning.\n",
      "Self-improvement. Below, we describe some\n",
      "widely-used datasets, and for full collected datasets\n",
      "we put them in Appendix A.\n",
      "3.1 Human-crafted Data\n",
      "Human-crafted data encompasses datasets that\n",
      "are either manually annotated or sourced directly\n",
      "from the internet. The creation of these datasets\n",
      "typically involves no machine learning techniques,\n",
      "relying solely on manual gathering and verification,\n",
      "resulting in generally smaller datasets. Below are\n",
      "some widely-used human-crafted datasets:\n",
      "3.1.1 Natural Instructions\n",
      "Natural Instructions (Mishra et al., 2021) is\n",
      "a human-crafted English instruction dataset\n",
      "consisting of 193K instances, coming from 61\n",
      "distinct NLP tasks. The dataset is comprised of\n",
      "\"instructions\" and \"instances\". Each instance in\n",
      "the \"instructions\" is a task description consisting\n",
      "of 7 components: title, definition, things to avoid\n",
      "emphasis/caution, prompt, positive example, and\n",
      "negative example. Subfigure (a) in Figure 2 gives\n",
      "an example of the \"instructions\". \"Instances\"\n",
      "consists of (\"input\", \"output\") pairs, which are the\n",
      "input data and textual result that follows the given\n",
      "instruction correctly. Subfigure (b) in Figure 2\n",
      "gives an example of the instances.\n",
      "The data comes from existing NLP datasets of\n",
      "61 tasks. The authors collected the \"instructions\"\n",
      "by referring to the dataset annotating instruction\n",
      "file. Next, the authors constructed the \"instances\"\n",
      "by unifying data instances across all NLP datasets\n",
      "to (\"input\", \"output\") pairs.\n",
      "3.1.2 P3\n",
      "P3 (Public Pool of Prompts) (Sanh et al., 2021)\n",
      "is an instruction tuning dataset constructed by\n",
      "integrating 170 English NLP datasets and 2,052\n",
      "English prompts. Prompts, which are sometimes\n",
      "named task templates, are functions that map a data\n",
      "instance in a conventional NLP task (e.g., question\n",
      "answering, text classification) to a natural language\n",
      "input-output pair.\n",
      "Each instance in P3 has three components:\n",
      "\"inputs\", \"answer_choices\", and “targets\". \"Inputs\"\n",
      "is a sequence of text that describes the task in\n",
      "natural language (e.g., \"If he like Mary is true, is\n",
      "it also true that he like Mary’s cat?\" ). \"Answer\n",
      "choices\" is a list of text string that are applicable\n",
      "responses to the given task (e.g., [\"yes\", \"no\",\n",
      "\"undetermined\"]). \"Targets\" is a text string that\n",
      "is the correct response to the given \"inputs\" (e.g.,\n",
      "\"yes\"). The authors built PromptSource, a tool for\n",
      "creating high-quality prompts collaboratively and\n",
      "an archive for open-sourcing high-quality prompts.\n",
      "The P3 dataset was built by randomly sampling a\n",
      "prompt from multiple prompts in the PromptSource\n",
      "and mapping each instance into a (\"inputs\",\n",
      "\"answer choices\", \"targets\") triplet.\n",
      "3.1.3 xP3\n",
      "xP3 (Crosslingual Public Pool of\n",
      "Prompts) (Muennighoff et al., 2022) is a\n",
      "multilingual instruction dataset consisting of 16\n",
      "diverse natural language tasks in 46 languages.\n",
      "Each instance in the dataset has two components:\n",
      "\"inputs\" and \"targets\". \"Inputs\" is a task description\n",
      "in natural language. \"Targets\" is the textual result\n",
      "that follows the \"inputs\" instruction correctly.\n",
      "The original data in xP3 comes from three\n",
      "sources: the English instruction dataset P3, 4\n",
      "English unseen tasks in P3 (e.g., translation,\n",
      "program synthesis), and 30 multilingual NLP\n",
      "datasets. The authors built the xP3 dataset\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "Instruction Tuning involves the process of fine-tuning a pre-trained model in a fully-supervised manner based on collected Supervised Fine-Tuning (SFT) datasets. These datasets consist of instructions, optional inputs for context, and anticipated outputs based on the instructions and inputs. The general pipeline of instruction tuning includes two main steps: Instruction Dataset Construction and Instruction Tuning/Supervised Fine-tuning.\n",
      "\n",
      "1. Instruction Dataset Construction:\n",
      "   - Instructions are natural language text sequences specifying tasks.\n",
      "   - Optional inputs provide supplementary context information.\n",
      "   - Anticipated outputs are based on instructions and inputs.\n",
      "   \n",
      "There are two methods for constructing instruction datasets:\n",
      "   - Data integration from annotated natural language datasets.\n",
      "   - Generating outputs using Large Language Models (LLMs) like GPT-3.5-Turbo or GPT-4.\n",
      "\n",
      "2. Instruction Tuning/Supervised Fine-tuning:\n",
      "   - Based on the collected SFT dataset, a pre-trained model is fine-tuned in a fully-supervised manner by predicting each token in the output sequentially.\n",
      "\n",
      "Additionally, the process of instruction tuning involves sections like methodology, construction of SFT representative datasets, representative instruction-tuned models, multi-modality techniques and datasets, adapting LLMs to different domains, efficient instruction tuning, and evaluation of SFT models.\n",
      "\n",
      "Would you like to know more details or information on any specific aspect of Instruction Tuning?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy.initiate_chat(\n",
    "    assistant,\n",
    "    message=\"Based on https://arxiv.org/pdf/2308.10792 paper, could you tell me: What is Instruction Tuning?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "def retriever_2(pdf_url: str, query: str) -> str:\n",
    "    docs = PyPDFLoader(pdf_url, extract_images=True).load()\n",
    "    chroma_db = Chroma.from_documents(docs, embedding=HuggingFaceEmbeddings())\n",
    "\n",
    "    chroma_retriever = chroma_db.as_retriever(\n",
    "        search_type=\"similarity\",\n",
    "        search_kwargs={\"k\":2}\n",
    "    )\n",
    "    retrieved_docs = chroma_retriever.invoke(query)\n",
    "\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_2 = ConversableAgent(\n",
    "    name=\"Assistant\",\n",
    "    system_message=\"You are a helpful AI assistant. \"\n",
    "    \"You can help answering questions based on input documents. \"\n",
    "    \"Return 'TERMINATE' when the task is done.\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-3.5-turbo\", \"temperature\": 0.9, \"api_key\": API_key}]},\n",
    ")\n",
    "\n",
    "user_proxy_2 = ConversableAgent(\n",
    "    name=\"User\",\n",
    "    llm_config=False,\n",
    "    is_termination_msg=lambda msg: msg.get(\"content\") is not None and \"TERMINATE\" in msg[\"content\"],\n",
    "    human_input_mode=\"NEVER\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import register_function\n",
    "\n",
    "register_function(\n",
    "    retriever_2,\n",
    "    caller=assistant_2,\n",
    "    executor=user_proxy_2,\n",
    "    name=\"retriever_2\",\n",
    "    description=\"A retriever who can return the relevant documents for a query based on a pdf document\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "Based on https://arxiv.org/pdf/2308.10792 paper, could you tell me: What is Instruction Tuning?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "\u001b[32m***** Suggested tool call (call_Vra4dGahaSlpeHgkWoeNcsop): retriever_2 *****\u001b[0m\n",
      "Arguments: \n",
      "{\"pdf_url\":\"https://arxiv.org/pdf/2308.10792\",\"query\":\"Instruction Tuning\"}\n",
      "\u001b[32m****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION retriever_2...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l7/y6ccq9mj31bgsmj26c6s97mw0000gn/T/ipykernel_59199/322295965.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  chroma_db = Chroma.from_documents(docs, embedding=HuggingFaceEmbeddings())\n",
      "/var/folders/l7/y6ccq9mj31bgsmj26c6s97mw0000gn/T/ipykernel_59199/322295965.py:5: LangChainDeprecationWarning: Default values for HuggingFaceEmbeddings.model_name were deprecated in LangChain 0.2.16 and will be removed in 0.4.0. Explicitly pass a model_name to the HuggingFaceEmbeddings constructor instead.\n",
      "  chroma_db = Chroma.from_documents(docs, embedding=HuggingFaceEmbeddings())\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a708df1f7bc54e3cabfcf364820f64c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1922f7fbe3104a1ab8d80bf6d3ee623b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1406d6cfbde24ee18d028f7e06fe504b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afe9c9a09a5945c5ae8b68110e2006b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ec29363dca04d54aed80210bacd7a9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b97efd8da7646cdb4b538c724f7247e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036dfbee872a4efd8519f4605cc6ecc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18ef42c958334eb09c53f77c7ac94eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904839691dbf43cb966b63f9d75b9d66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e5661112694449da54c93bd041bff9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5ddd2fe5b0649f690a8799d11e5d662",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_Vra4dGahaSlpeHgkWoeNcsop) *****\u001b[0m\n",
      "Error: Embedding dimension 768 does not match collection dimensionality 1024\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "I encountered an error while trying to retrieve information from the document. Let me try again to provide you with the details about Instruction Tuning from the paper.\n",
      "\u001b[32m***** Suggested tool call (call_YSb89kxuJ22UOtD5siDsEaH2): retriever_2 *****\u001b[0m\n",
      "Arguments: \n",
      "{\"pdf_url\":\"https://arxiv.org/pdf/2308.10792\",\"query\":\"Instruction Tuning\"}\n",
      "\u001b[32m****************************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTING FUNCTION retriever_2...\u001b[0m\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\u001b[32m***** Response from calling tool (call_YSb89kxuJ22UOtD5siDsEaH2) *****\u001b[0m\n",
      "Error: Embedding dimension 768 does not match collection dimensionality 1024\n",
      "\u001b[32m**********************************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "I apologize for the inconvenience. It seems there is a technical issue preventing me from retrieving the information about Instruction Tuning from the document. Would you like me to try again or assist you with anything else?\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mUser\u001b[0m (to Assistant):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mAssistant\u001b[0m (to User):\n",
      "\n",
      "TERMINATE\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "chat_result = user_proxy_2.initiate_chat(\n",
    "    assistant_2,\n",
    "    message=\"Based on https://arxiv.org/pdf/2308.10792 paper, could you tell me: What is Instruction Tuning?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
